{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "import pandas as pd\n",
    "data = pd.read_csv('airbnb_test.csv', parse_dates=['host_since'])\n",
    "\n",
    "data = data.drop(columns=['listing_url', 'scrape_id', 'last_scraped', 'name_x', 'summary', 'space', 'description', 'neighborhood_overview'])\n",
    "data = data.drop(columns=['notes', 'transit', 'access', 'interaction', 'house_rules', 'thumbnail_url', 'medium_url', 'picture_url',])\n",
    "data = data.drop(columns=['xl_picture_url', 'host_id_x', 'host_url', 'host_name_x', 'host_acceptance_rate', 'host_thumbnail_url', 'host_picture_url'])\n",
    "data = data.drop(columns=['host_about', 'host_listings_count', 'country', 'calendar_last_scraped', 'requires_license', 'license'])\n",
    "data = data.drop(columns=['jurisdiction_names', 'host_id_y', 'host_name_y', 'host_location', 'street', 'city', 'state', 'country_code'])\n",
    "data = data.drop(columns=['smart_location', 'square_feet', 'name_y', 'latitude_y', 'longitude_y', 'room_type_y', 'experiences_offered'])\n",
    "data = data.drop(columns=['calendar_updated', 'first_review', 'last_review_y'])\n",
    "data = data.drop(columns=['market', 'require_guest_profile_picture', 'reviews_per_month_y', 'calculated_host_listings_count_y'])\n",
    "data = data.drop(columns=['availability_365_y', 'host_verifications', 'date'])\n",
    "# Location data\n",
    "data = data.drop(columns=['host_neighbourhood', 'neighbourhood_x', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'zipcode', 'neighbourhood_y', 'neighbourhood_group'])\n",
    "# data = data.drop(columns=['latitude_x', 'longitude_x', 'is_location_exact'])\n",
    "# availability data\n",
    "data = data.drop(columns=['availability_365_x', 'availability_30', 'availability_60', 'available', 'has_availability'])\n",
    "# response data\n",
    "data = data.drop(columns=['host_response_time', 'host_response_rate'])\n",
    "# review data\n",
    "data = data.drop(columns=['last_review_x'])\n",
    "\n",
    "\n",
    "data['cancellation_policy_mapped'] = data.cancellation_policy.map({'super_strict_60': 0, 'strict_14_with_grace_period':1, 'moderate':2, 'flexible':3})\n",
    "data = data.drop(columns=['cancellation_policy'])\n",
    "data['bed_code'] = data.bed_type.map({'Real Bed':2, 'Pull-out Sofa':1, 'Futon':0})\n",
    "data = data.drop(columns=['bed_type'])\n",
    "data['room_code'] = data.room_type_x.map({'Entire home/apt':1, 'Private room':0, 'Shared room':2})\n",
    "data = data.drop(columns=['room_type_x'])\n",
    "data['verified_id'] = data.host_identity_verified.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['host_identity_verified'])\n",
    "data['profile_pic'] = data.host_has_profile_pic.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['host_has_profile_pic'])\n",
    "data['superhost'] = data.host_is_superhost.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['host_is_superhost'])\n",
    "data['phone_verified'] = data.require_guest_phone_verification.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['require_guest_phone_verification'])\n",
    "data['buisiness_travel'] = data.is_business_travel_ready.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['is_business_travel_ready'])\n",
    "data['instant_book'] = data.instant_bookable.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['instant_bookable'])\n",
    "\n",
    "test_data = data\n",
    "data.to_csv('airbnb_test_coded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pouya/anaconda3/envs/python38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (2,15,16,18,27,33,43,53,54,65,67,68,72,74,76,79,83,88) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import pandas as pd\n",
    "data = pd.read_csv('airbnb_merged.csv')\n",
    "\n",
    "data = data.drop(columns=['listing_url', 'scrape_id', 'last_scraped', 'name_x', 'summary', 'space', 'description', 'neighborhood_overview'])\n",
    "data = data.drop(columns=['notes', 'transit', 'access', 'interaction', 'house_rules', 'thumbnail_url', 'medium_url', 'picture_url',])\n",
    "data = data.drop(columns=['xl_picture_url', 'host_id_x', 'host_url', 'host_name_x', 'host_acceptance_rate', 'host_thumbnail_url', 'host_picture_url'])\n",
    "data = data.drop(columns=['host_about', 'host_listings_count', 'country', 'calendar_last_scraped', 'requires_license', 'license'])\n",
    "data = data.drop(columns=['jurisdiction_names', 'host_id_y', 'host_name_y', 'host_location', 'street', 'city', 'state', 'country_code'])\n",
    "data = data.drop(columns=['smart_location', 'square_feet', 'name_y', 'latitude_y', 'longitude_y', 'room_type_y', 'price_y', 'experiences_offered'])\n",
    "data = data.drop(columns=['weekly_price', 'monthly_price', 'calendar_updated', 'first_review', 'last_review_y'])\n",
    "data = data.drop(columns=['market', 'require_guest_profile_picture', 'reviews_per_month_y', 'calculated_host_listings_count_y'])\n",
    "data = data.drop(columns=['availability_365_y', 'host_verifications', 'date'])\n",
    "# Location data\n",
    "data = data.drop(columns=['host_neighbourhood', 'neighbourhood_x', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'zipcode', 'neighbourhood_y', 'neighbourhood_group'])\n",
    "# data = data.drop(columns=['latitude_x', 'longitude_x', 'is_location_exact'])\n",
    "# availability data\n",
    "data = data.drop(columns=['availability_365_x', 'availability_30', 'availability_60', 'available', 'has_availability'])\n",
    "# response data\n",
    "data = data.drop(columns=['host_response_time', 'host_response_rate'])\n",
    "# review data\n",
    "data = data.drop(columns=['last_review_x'])\n",
    "\n",
    "data.to_csv('airbnb_cleaned.csv')\n",
    "\n",
    "data['cancellation_policy_mapped'] = data.cancellation_policy.map({'super_strict_60': 0, 'strict_14_with_grace_period':1, 'moderate':2, 'flexible':3})\n",
    "data = data.drop(columns=['cancellation_policy'])\n",
    "data['bed_code'] = data.bed_type.map({'Real Bed':2, 'Pull-out Sofa':1, 'Futon':0})\n",
    "data = data.drop(columns=['bed_type'])\n",
    "data['room_code'] = data.room_type_x.map({'Entire home/apt':1, 'Private room':0, 'Shared room':2})\n",
    "data = data.drop(columns=['room_type_x'])\n",
    "data['verified_id'] = data.host_identity_verified.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['host_identity_verified'])\n",
    "data['profile_pic'] = data.host_has_profile_pic.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['host_has_profile_pic'])\n",
    "data['superhost'] = data.host_is_superhost.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['host_is_superhost'])\n",
    "data['phone_verified'] = data.require_guest_phone_verification.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['require_guest_phone_verification'])\n",
    "data['buisiness_travel'] = data.is_business_travel_ready.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['is_business_travel_ready'])\n",
    "data['instant_book'] = data.instant_bookable.map({'t':1, 'f':0})\n",
    "data = data.drop(columns=['instant_bookable'])\n",
    "\n",
    "data.to_csv('airbnb_coded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pouya/anaconda3/envs/python38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (3,8,9,16,18,19,21,22,26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "import hdbscan\n",
    "\n",
    "big_data = pd.read_csv('airbnb_coded.csv')\n",
    "sc = sc()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# data['property_types'] = data.property_type.map({'Apartment':0, 'House':1, 'Loft':2, 'Hotel':3, 'Condominium':4, 'Tiny house':5, 'Serviced apartment':6, 'Boutique hotel':7,\n",
    "#                                            'Guest suit':8, 'Houseboat':9, 'Hostel':10, 'Villa':11, 'Townhouse':12, 'Cabin':13, 'Camper/RV':14})\n",
    "# data['property_types'] = data.property_type.map({'Apartment':0, 'House':1, 'Loft':2, 'Hotel':3, 'Condominium':4, 'Tiny house':5, 'Serviced apartment':6, 'Boutique hotel':7,\n",
    "#                                            'Guest suit':8, 'Houseboat':9, 'Hostel':10, 'Villa':11, 'Townhouse':12, 'Cabin':13, 'Camper/RV':14})\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "var = 'bathrooms'\n",
    "var2 = 'accommodates'\n",
    "var3 = 'cleaning_fee'\n",
    "var10 = 'price_x'\n",
    "\n",
    "data = big_data[['property_type', var, var2, var3, var10]]#, 'cleaning_fee', 'security_deposit']]\n",
    "# data['cleaning_fee'] = data['cleaning_fee'].fillna(data['cleaning_fee'].median())\n",
    "\n",
    "\n",
    "data[var3] = data[var3].str.replace(',', '')\n",
    "data[var3] = data[var3].str.replace('$', '')\n",
    "\n",
    "big_data[var3] = big_data[var3].str.replace(',', '')\n",
    "big_data[var3] = big_data[var3].str.replace('$', '')\n",
    "\n",
    "data[var10] = data[var10].str.replace(',', '')\n",
    "data[var10] = data[var10].str.replace('$', '')\n",
    "\n",
    "big_data[var10] = big_data[var10].str.replace(',', '')\n",
    "big_data[var10] = big_data[var10].str.replace('$', '')\n",
    "\n",
    "\n",
    "data[var][6722] = 0\n",
    "data[var][4476] = 0\n",
    "data[var2][6722] = 0\n",
    "data[var2][4476] = 0\n",
    "data[var3][6722] = 0\n",
    "data[var3][4476] = 0\n",
    "# data['security_deposit'][4476] = 0\n",
    "# data['security_deposit'] = data['security_deposit'].fillna(data['security_deposit'].median())\n",
    "data[var] = data[var].fillna(data[var].median())\n",
    "data[var2] = data[var2].fillna(data[var2].median())\n",
    "data[var3] = data[var3].fillna(data[var3].median())\n",
    "data[var10] = data[var10].fillna(data[var10].median())\n",
    "data['property_type'] = data['property_type'].fillna('Apartment')\n",
    "# data.head()\n",
    "data = data.drop(data.index[14973])\n",
    "big_data = big_data.drop(data.index[14973])\n",
    "big_data = big_data.drop(big_data.index[6722])\n",
    "big_data = big_data.drop(big_data.index[4476])\n",
    "data = data.drop(data.index[6722])\n",
    "data = data.drop(data.index[4476])\n",
    "X = data.iloc[:, 1:-1].values\n",
    "# X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "# X = np.array(X)\n",
    "# X1 = data[var2].values #, data['cleaning_fee'].values, data['security_deposit'].values\n",
    "# # X2 = \n",
    "# y = data['property_type'].values\n",
    "# cluster_data = []\n",
    "# for i in range(len(X1)):\n",
    "#     cluster_data.append([y[i], X1[i]])\n",
    "# cluster_data = np.array(cluster_data)\n",
    "# # cluster_data = sc.fit_transform(cluster_data)\n",
    "# print(len(cluster_data))\n",
    "\n",
    "clusterer = cluster.KMeans(n_clusters=4)\n",
    "X = clusterer.fit_predict(X)\n",
    "# clusterer.fit(X)\n",
    "# Clusters = clusterer.predict(X)\n",
    "data['listing_class'] = X\n",
    "big_data['listing_class'] = X\n",
    "\n",
    "\n",
    "data = data[['property_type', var, var2, var3, 'listing_class', var10]]\n",
    "X = data.iloc[:, 1:-2].values\n",
    "y = data.iloc[:, -2].values\n",
    "# X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "# X = np.array(X)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=500, min_samples_leaf=1)\n",
    "classifier = classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "import hdbscan\n",
    "\n",
    "# test_data = pd.read_csv('airbnb_test_coded.csv', parse_dates=['host_since'])\n",
    "sc = sc()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# data['property_types'] = data.property_type.map({'Apartment':0, 'House':1, 'Loft':2, 'Hotel':3, 'Condominium':4, 'Tiny house':5, 'Serviced apartment':6, 'Boutique hotel':7,\n",
    "#                                            'Guest suit':8, 'Houseboat':9, 'Hostel':10, 'Villa':11, 'Townhouse':12, 'Cabin':13, 'Camper/RV':14})\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "var = 'bathrooms'\n",
    "var2 = 'accommodates'\n",
    "var3 = 'cleaning_fee'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = test_data[['property_type', var, var2, var3]]\n",
    "\n",
    "\n",
    "data[var3] = data[var3].str.replace(',', '')\n",
    "data[var3] = data[var3].str.replace('$', '')\n",
    "\n",
    "test_data[var3] = test_data[var3].str.replace(',', '')\n",
    "test_data[var3] = test_data[var3].str.replace('$', '')\n",
    "\n",
    "test_data['host_since'] = test_data['host_since'].str.replace('DE', '2014-04-16')\n",
    "# test_data['host_since'] = test_data['host_since'].str.replace('', '2014-04-16')\n",
    "\n",
    "\n",
    "data[var][3105] = 0\n",
    "data[var][4476] = 0\n",
    "data[var2][3105] = 0\n",
    "data[var2][4476] = 0\n",
    "data[var3][3105] = 0\n",
    "data[var3][4476] = 0\n",
    "\n",
    "\n",
    "data[var] = data[var].fillna(data[var].median())\n",
    "data[var2] = data[var2].fillna(data[var2].median())\n",
    "data[var3] = data[var3].fillna(data[var3].median())\n",
    "data['property_type'] = data['property_type'].fillna('Apartment')\n",
    "\n",
    "X = data.iloc[:, 1:].values\n",
    "# X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "# X = np.array(X)\n",
    "\n",
    "\n",
    "\n",
    "data = data[['property_type', var, var2, var3]]\n",
    "X = data.iloc[:, 1:].values\n",
    "# X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "# X = np.array(X)\n",
    "\n",
    "y_pred = classifier.predict(X)\n",
    "\n",
    "data['listing_class'] = y_pred\n",
    "test_data['listing_class'] = y_pred\n",
    "\n",
    "big_data.to_csv('airbnb_coded.2.0.csv')\n",
    "test_data.to_csv('airbnb_test_coded.csv')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.4462961  52.48141516]\n",
      " [13.41708604 52.60610923]\n",
      " [13.50276362 52.44614449]\n",
      " ...\n",
      " [13.29098912 52.4976456 ]\n",
      " [13.38508885 52.46504928]\n",
      " [13.58954646 52.46768108]]\n",
      "clustered\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "import hdbscan\n",
    "\n",
    "big_data = pd.read_csv('airbnb_coded.2.0.csv', parse_dates=['host_since'])\n",
    "sc = sc()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
    "var = 'bathrooms'\n",
    "var2 = 'accommodates'\n",
    "var3 = 'cleaning_fee'\n",
    "var4 = 'bedrooms'\n",
    "var5 = 'beds'\n",
    "var6 = 'minimum_nights_x'\n",
    "var7 = 'maximum_nights'\n",
    "var8 = 'number_of_reviews_y'\n",
    "var9 = 'listing_class'\n",
    "var10 = 'price_x'\n",
    "var11 = 'bed_code'\n",
    "var12 = 'cancellation_policy_mapped'\n",
    "var13 = 'room_code'\n",
    "var14 = 'superhost'\n",
    "var15 = 'instant_book'\n",
    "var16 = 'review_scores_location'\n",
    "var17 = 'review_scores_rating'\n",
    "var18 = 'host_total_listings_count'\n",
    "var19 = 'availability_90'\n",
    "var20 = 'room_code'\n",
    "var21 = 'extra_people'\n",
    "var22 = 'property_type'\n",
    "var23 = 'review_scores_value'\n",
    "var24 = 'review_scores_cleanliness'\n",
    "var25 = 'review_scores_checkin'\n",
    "var26 = 'review_scores_communication'\n",
    "var27 = 'review_scores_value'\n",
    "var28 = 'guests_included'\n",
    "var29 = 'security_deposit'\n",
    "var30 = 'host_since'\n",
    "var31 = 'longitude_x'\n",
    "var32 = 'latitude_x'\n",
    "var33 = 'amenities'\n",
    "\n",
    "# big_data = big_data[big_data.price_x < 500]\n",
    "\n",
    "data = big_data[[var, var2, var3, var4, var5, var6, var7, var8, var11, var12, var13, var14, var15, var16, var17, var18, var19, var20, var21, var22, var23, var24,\n",
    "                 var25, var26, var27, var28, var29, var30, var31, var32, var33, var9, var10]]\n",
    "\n",
    "\n",
    "\n",
    "data[var21] = data[var21].str.replace(',', '')\n",
    "data[var21] = data[var21].str.replace('$', '')\n",
    "\n",
    "big_data[var21] = big_data[var21].str.replace(',', '')\n",
    "big_data[var21] = big_data[var21].str.replace('$', '')\n",
    "\n",
    "data[var29] = data[var29].str.replace(',', '')\n",
    "data[var29] = data[var29].str.replace('$', '')\n",
    "\n",
    "big_data[var29] = big_data[var29].str.replace(',', '')\n",
    "big_data[var29] = big_data[var29].str.replace('$', '')\n",
    "\n",
    "\n",
    "data[var30] = (big_data.host_since - big_data.host_since.min())\n",
    "data[var30] = data[var30].values.astype('timedelta64[D]').astype('float')\n",
    "\n",
    "big_data[var30] = (big_data.host_since - big_data.host_since.min())\n",
    "big_data[var30] = big_data[var30].values.astype('timedelta64[D]').astype('float')\n",
    "\n",
    "\n",
    "data[var] = data[var].fillna(data[var].median())\n",
    "data[var2] = data[var2].fillna(data[var2].median())\n",
    "data[var3] = data[var3].fillna(data[var3].median())\n",
    "data[var4] = data[var4].fillna(data[var4].median())\n",
    "data[var5] = data[var5].fillna(data[var5].median())\n",
    "\n",
    "data[var6][17990] = data[var6].median()\n",
    "data[var6][6215] = data[var6].median()\n",
    "data[var6][11299] = data[var6].median()\n",
    "data[var6][12737] = data[var6].median()\n",
    "data[var6][14598] = data[var6].median()\n",
    "# data[var7][872] = 8000\n",
    "# data[var7][2372] = 8000\n",
    "# data[var7][4391] = 8000\n",
    "# data[var7][17952] = 8000\n",
    "# data[var7][17990] = data[var7].median()\n",
    "# data[var7][6351] = data[var7].median()\n",
    "# data[var7][8341] = data[var7].median()\n",
    "\n",
    "\n",
    "data[var6] = data[var6].fillna(data[var6].median())\n",
    "data[var7] = data[var7].fillna(data[var7].median())\n",
    "data[var8] = data[var8].fillna(data[var8].median())\n",
    "data[var10] = data[var10].fillna(data[var10].median())\n",
    "data[var11] = data[var11].fillna(data[var11].median())\n",
    "data[var12] = data[var12].fillna(data[var12].median())\n",
    "data[var13] = data[var13].fillna(data[var13].median())\n",
    "data[var14] = data[var14].fillna(data[var14].median())\n",
    "data[var15] = data[var15].fillna(data[var15].median())\n",
    "data[var16] = data[var16].fillna(data[var16].median())\n",
    "data[var17] = data[var17].fillna(data[var17].median())\n",
    "data[var18] = data[var18].fillna(data[var18].median())\n",
    "data[var19] = data[var19].fillna(data[var19].median())\n",
    "data[var20] = data[var20].fillna(data[var20].median())\n",
    "data[var21] = data[var21].fillna(data[var21].median())\n",
    "data[var22] = data[var22].fillna('Apartment')\n",
    "data[var23] = data[var23].fillna(data[var23].median())\n",
    "data[var24] = data[var24].fillna(data[var24].median())\n",
    "data[var25] = data[var25].fillna(data[var25].median())\n",
    "data[var26] = data[var26].fillna(data[var26].median())\n",
    "data[var27] = data[var27].fillna(data[var27].median())\n",
    "data[var28] = data[var28].fillna(data[var28].median())\n",
    "data[var29] = data[var29].fillna(data[var29].median())\n",
    "data[var30] = data[var30].fillna(data[var30].median())\n",
    "data[var31] = data[var31].fillna(data[var31].median())\n",
    "data[var32] = data[var32].fillna(data[var32].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data[var6][17990] = data[var6].median()\n",
    "\n",
    "# data = data.drop(data.index[17990])\n",
    "# data = data.drop(data.index[14598])\n",
    "\n",
    "data = data[[var31, var32, var10]]\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "# y = data.iloc[:, -1].values\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-4])], remainder='passthrough')\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-5])], remainder='passthrough')\n",
    "# X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "# X = np.array(X)\n",
    "# ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-6])], remainder='passthrough')\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "\n",
    "# plt.scatter(data[var6].values, data[var7].values, s=100, c='red', alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "print(X)\n",
    "# X[:,-8:] = sc.fit_transform(X[:,-8:])\n",
    "# X = sc.fit_transform(X)\n",
    "# print(X[0])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# data.head()\n",
    "\n",
    "\n",
    "# label = hdbscan.HDBSCAN(min_cluster_size=450, min_samples=15).fit_predict(cluster_data)\n",
    "clusterer = cluster.KMeans(n_clusters=4)\n",
    "X = clusterer.fit_predict(X)\n",
    "# clusterer.fit(X)\n",
    "# Clusters = clusterer.predict(X)\n",
    "data['loc_class'] = X\n",
    "big_data['loc_class'] = X\n",
    "print(\"clustered\")\n",
    "\n",
    "data = data[[var31, var32, 'loc_class', var10]]\n",
    "X = data.iloc[:, :-2].values\n",
    "y = data.iloc[:, -2].values\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=500, min_samples_leaf=1)\n",
    "classifier = classifier.fit(X, y)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.43573367 52.47111954]\n",
      " [13.36623169 52.50338753]\n",
      " [13.30064836 52.50791325]\n",
      " ...\n",
      " [13.45286474 52.51908785]\n",
      " [13.40887987 52.53712419]\n",
      " [13.41186395 52.49068768]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "import hdbscan\n",
    "\n",
    "test_data = pd.read_csv('airbnb_test_coded.csv', parse_dates=['host_since'])\n",
    "sc = sc()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
    "var = 'bathrooms'\n",
    "var2 = 'accommodates'\n",
    "var3 = 'cleaning_fee'\n",
    "var4 = 'bedrooms'\n",
    "var5 = 'beds'\n",
    "var6 = 'minimum_nights_x'\n",
    "var7 = 'maximum_nights'\n",
    "var8 = 'number_of_reviews_y'\n",
    "var11 = 'bed_code'\n",
    "var12 = 'cancellation_policy_mapped'\n",
    "var13 = 'room_code'\n",
    "var14 = 'superhost'\n",
    "var15 = 'instant_book'\n",
    "var16 = 'review_scores_location'\n",
    "var17 = 'review_scores_rating'\n",
    "var18 = 'host_total_listings_count'\n",
    "var19 = 'availability_90'\n",
    "var20 = 'room_code'\n",
    "var21 = 'extra_people'\n",
    "var22 = 'property_type'\n",
    "var23 = 'review_scores_value'\n",
    "var24 = 'review_scores_cleanliness'\n",
    "var25 = 'review_scores_checkin'\n",
    "var26 = 'review_scores_communication'\n",
    "var27 = 'review_scores_value'\n",
    "var28 = 'guests_included'\n",
    "var29 = 'security_deposit'\n",
    "var30 = 'host_since'\n",
    "var31 = 'longitude_x'\n",
    "var32 = 'latitude_x'\n",
    "var33 = 'amenities'\n",
    "\n",
    "# big_data = test_data[big_data.price_x < 500]\n",
    "\n",
    "data = test_data[[var, var2, var3, var4, var5, var6, var8, var11, var12, var13, var14, var15, var16, var17, var18, var19, var20, var21, var22, var23, var24,\n",
    "                 var25, var26, var27, var28, var29, var30, var31, var32, var33]]\n",
    "\n",
    "\n",
    "data[var17] = data[var17].replace('Shared room', 5)\n",
    "data[var] = data[var].replace('2018-10-31', 0)\n",
    "data[var2] = data[var2].replace('2018-07-20', 0)\n",
    "data[var6] = data[var6].replace('moderate', 15)\n",
    "data[var18] = data[var6].replace('Real Bed', 1)\n",
    "data[var21] = data[var21].str.replace(',', '')\n",
    "data[var21] = data[var21].str.replace('$', '')\n",
    "data[var29] = data[var29].str.replace(',', '')\n",
    "data[var29] = data[var29].str.replace('$', '')\n",
    "data[var21] = data[var21].replace('f', 0)\n",
    "data[var26] = data[var26].replace('2018-10-31', 5)\n",
    "data[var28] = data[var28].replace('f', 0)\n",
    "\n",
    "\n",
    "\n",
    "test_data[var17] = test_data[var17].replace('Shared room', 5)\n",
    "test_data[var] = test_data[var].replace('2018-10-31', 0)\n",
    "test_data[var2] = test_data[var2].replace('2018-07-20', 0)\n",
    "test_data[var6] = test_data[var6].replace('moderate', 15)\n",
    "test_data[var18] = test_data[var6].replace('Real Bed', 1)\n",
    "test_data[var29] = test_data[var29].str.replace(',', '')\n",
    "test_data[var29] = test_data[var29].str.replace('$', '')\n",
    "test_data[var21] = test_data[var21].str.replace(',', '')\n",
    "test_data[var21] = test_data[var21].str.replace('$', '')\n",
    "test_data[var21] = test_data[var21].replace('f', 0)\n",
    "test_data[var26] = test_data[var26].replace('2018-10-31', 5)\n",
    "test_data[var28] = test_data[var28].replace('f', 0)\n",
    "\n",
    "data[var30] = (test_data.host_since - test_data.host_since.min())\n",
    "data[var30] = data[var30].values.astype('timedelta64[D]').astype('float')\n",
    "\n",
    "test_data[var30] = (test_data.host_since - test_data.host_since.min())\n",
    "test_data[var30] = test_data[var30].values.astype('timedelta64[D]').astype('float')\n",
    "\n",
    "\n",
    "data[var][4476] = 0\n",
    "data[var2][3105] = 0\n",
    "data[var2][4476] = 0\n",
    "data[var3][3105] = 0\n",
    "data[var3][4476] = 0\n",
    "\n",
    "\n",
    "data[var] = data[var].fillna(data[var].median())\n",
    "data[var2] = data[var2].fillna(data[var2].median())\n",
    "data[var3] = data[var3].fillna(data[var3].median())\n",
    "data[var4] = data[var4].fillna(data[var4].median())\n",
    "data[var5] = data[var5].fillna(data[var5].median())\n",
    "\n",
    "data[var6][17990] = data[var6].median()\n",
    "data[var6][6215] = data[var6].median()\n",
    "data[var6][11299] = data[var6].median()\n",
    "data[var6][12737] = data[var6].median()\n",
    "data[var6][14598] = data[var6].median()\n",
    "# data[var7][872] = 8000\n",
    "# data[var7][2372] = 8000\n",
    "# data[var7][4391] = 8000\n",
    "# data[var7][17952] = 8000\n",
    "# data[var7][17990] = data[var7].median()\n",
    "# data[var7][6351] = data[var7].median()\n",
    "# data[var7][8341] = data[var7].median()\n",
    "\n",
    "\n",
    "data[var6] = data[var6].fillna(data[var6].median())\n",
    "# data[var7] = data[var7].fillna(data[var7].median())\n",
    "data[var8] = data[var8].fillna(data[var8].median())\n",
    "# data[var10] = data[var10].fillna(data[var10].median())\n",
    "data[var11] = data[var11].fillna(data[var11].median())\n",
    "data[var12] = data[var12].fillna(data[var12].median())\n",
    "data[var13] = data[var13].fillna(data[var13].median())\n",
    "data[var14] = data[var14].fillna(data[var14].median())\n",
    "data[var15] = data[var15].fillna(data[var15].median())\n",
    "data[var16] = data[var16].fillna(data[var16].median())\n",
    "data[var17] = data[var17].fillna(data[var17].median())\n",
    "data[var18] = data[var18].fillna(data[var18].median())\n",
    "data[var19] = data[var19].fillna(data[var19].median())\n",
    "data[var20] = data[var20].fillna(data[var20].median())\n",
    "data[var21] = data[var21].fillna(data[var21].median())\n",
    "data[var22] = data[var22].fillna('Apartment')\n",
    "data[var23] = data[var23].fillna(data[var23].median())\n",
    "data[var24] = data[var24].fillna(data[var24].median())\n",
    "data[var25] = data[var25].fillna(data[var25].median())\n",
    "data[var26] = data[var26].fillna(data[var26].median())\n",
    "data[var27] = data[var27].fillna(data[var27].median())\n",
    "data[var28] = data[var28].fillna(data[var28].median())\n",
    "data[var29] = data[var29].fillna(data[var29].median())\n",
    "data[var30] = data[var30].fillna(data[var30].median())\n",
    "data[var31] = data[var31].fillna(data[var31].median())\n",
    "data[var32] = data[var32].fillna(data[var32].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data[var6][17990] = data[var6].median()\n",
    "\n",
    "# data = data.drop(data.index[17990])\n",
    "# data = data.drop(data.index[14598])\n",
    "\n",
    "data = data[[var31, var32]]\n",
    "\n",
    "X = data.iloc[:, :].values\n",
    "# y = data.iloc[:, -1].values\n",
    "\n",
    "# plt.scatter(data[var6].values, data[var7].values, s=100, c='red', alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "print(X)\n",
    "# X[:,-8:] = sc.fit_transform(X[:,-8:])\n",
    "# X = sc.fit_transform(X)\n",
    "# print(X[0])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# data.head()\n",
    "\n",
    "\n",
    "# data = data[[var31, var32]]\n",
    "# X = data.iloc[:, :-1].values\n",
    "# X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "# X = np.array(X)\n",
    "y_pred = classifier.predict(X)\n",
    "\n",
    "data['loc_class'] = y_pred\n",
    "test_data['loc_class'] = y_pred\n",
    "\n",
    "big_data.to_csv('airbnb_coded.2.0.csv')\n",
    "test_data.to_csv('airbnb_test_coded.csv')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "[[  1.   0.   0. ...  97.   1. 100.]\n",
      " [  1.   0.   0. ...  75.   1. 100.]\n",
      " [  1.   0.   0. ...  80.   1.   0.]\n",
      " ...\n",
      " [  1.   0.   0. ...  99.   1.   0.]\n",
      " [  1.   0.   0. ...  90.   1.   0.]\n",
      " [  1.   0.   0. ... 100.   1. 100.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "<ipython-input-7-fab96fddf885>:174: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=26, units=17, kernel_initializer=\"uniform\")`\n",
      "  ann.add(Dense(output_dim=17, init='uniform', activation='relu', input_dim=26))\n",
      "<ipython-input-7-fab96fddf885>:175: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=17, kernel_initializer=\"uniform\")`\n",
      "  ann.add(Dense(output_dim=17, init='uniform', activation='relu'))\n",
      "<ipython-input-7-fab96fddf885>:176: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=17, kernel_initializer=\"uniform\")`\n",
      "  ann.add(Dense(output_dim=17, init='uniform', activation='relu'))\n",
      "<ipython-input-7-fab96fddf885>:177: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=17, kernel_initializer=\"uniform\")`\n",
      "  ann.add(Dense(output_dim=17, init='uniform', activation='relu'))\n",
      "<ipython-input-7-fab96fddf885>:178: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=17, kernel_initializer=\"uniform\")`\n",
      "  ann.add(Dense(output_dim=17, init='uniform', activation='relu'))\n",
      "<ipython-input-7-fab96fddf885>:179: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"uniform\")`\n",
      "  ann.add(Dense(output_dim=1, init='uniform'))\n",
      "<ipython-input-7-fab96fddf885>:181: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  ann.fit(X, y, batch_size=12, nb_epoch=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17930/17930 [==============================] - 2s 90us/step - loss: 1527.0010 - mean_squared_error: 1527.0024\n",
      "Epoch 2/100\n",
      "17930/17930 [==============================] - 1s 83us/step - loss: 1121.0281 - mean_squared_error: 1121.0278\n",
      "Epoch 3/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 961.0929 - mean_squared_error: 961.0925\n",
      "Epoch 4/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 925.9189 - mean_squared_error: 925.9184\n",
      "Epoch 5/100\n",
      "17930/17930 [==============================] - 1s 82us/step - loss: 910.7356 - mean_squared_error: 910.7359\n",
      "Epoch 6/100\n",
      "17930/17930 [==============================] - 2s 84us/step - loss: 892.3663 - mean_squared_error: 892.3672\n",
      "Epoch 7/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 881.5518 - mean_squared_error: 881.5519\n",
      "Epoch 8/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 868.0173 - mean_squared_error: 868.0181\n",
      "Epoch 9/100\n",
      "17930/17930 [==============================] - 2s 85us/step - loss: 849.5594 - mean_squared_error: 849.5598\n",
      "Epoch 10/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 838.6170 - mean_squared_error: 838.6171\n",
      "Epoch 11/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 849.0600 - mean_squared_error: 849.0597\n",
      "Epoch 12/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 841.2496 - mean_squared_error: 841.2493\n",
      "Epoch 13/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 836.3508 - mean_squared_error: 836.3506\n",
      "Epoch 14/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 828.0876 - mean_squared_error: 828.0877\n",
      "Epoch 15/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 835.2926 - mean_squared_error: 835.2924\n",
      "Epoch 16/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 847.5520 - mean_squared_error: 847.5517\n",
      "Epoch 17/100\n",
      "17930/17930 [==============================] - 1s 83us/step - loss: 815.2824 - mean_squared_error: 815.2826\n",
      "Epoch 18/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 820.6691 - mean_squared_error: 820.6689\n",
      "Epoch 19/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 811.6637 - mean_squared_error: 811.6640\n",
      "Epoch 20/100\n",
      "17930/17930 [==============================] - 1s 81us/step - loss: 826.5987 - mean_squared_error: 826.5980\n",
      "Epoch 21/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 808.8205 - mean_squared_error: 808.8204\n",
      "Epoch 22/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 801.0302 - mean_squared_error: 801.0299\n",
      "Epoch 23/100\n",
      "17930/17930 [==============================] - 1s 83us/step - loss: 815.3866 - mean_squared_error: 815.3865\n",
      "Epoch 24/100\n",
      "17930/17930 [==============================] - 1s 83us/step - loss: 810.2940 - mean_squared_error: 810.2940\n",
      "Epoch 25/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 804.8968 - mean_squared_error: 804.8965\n",
      "Epoch 26/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 797.6030 - mean_squared_error: 797.6030\n",
      "Epoch 27/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 795.4552 - mean_squared_error: 795.4554\n",
      "Epoch 28/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 797.8653 - mean_squared_error: 797.8653\n",
      "Epoch 29/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 798.5697 - mean_squared_error: 798.5698\n",
      "Epoch 30/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 792.2253 - mean_squared_error: 792.2259\n",
      "Epoch 31/100\n",
      "17930/17930 [==============================] - 1s 83us/step - loss: 796.5893 - mean_squared_error: 796.5895\n",
      "Epoch 32/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 800.7354 - mean_squared_error: 800.7355\n",
      "Epoch 33/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 782.3222 - mean_squared_error: 782.3228\n",
      "Epoch 34/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 799.4276 - mean_squared_error: 799.4268\n",
      "Epoch 35/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 782.9928 - mean_squared_error: 782.9932\n",
      "Epoch 36/100\n",
      "17930/17930 [==============================] - 1s 83us/step - loss: 785.1561 - mean_squared_error: 785.1561\n",
      "Epoch 37/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 783.2116 - mean_squared_error: 783.2114\n",
      "Epoch 38/100\n",
      "17930/17930 [==============================] - 2s 91us/step - loss: 785.4657 - mean_squared_error: 785.4656\n",
      "Epoch 39/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 785.1072 - mean_squared_error: 785.1075\n",
      "Epoch 40/100\n",
      "17930/17930 [==============================] - 2s 87us/step - loss: 776.3742 - mean_squared_error: 776.3744\n",
      "Epoch 41/100\n",
      "17930/17930 [==============================] - 1s 81us/step - loss: 774.7562 - mean_squared_error: 774.7558\n",
      "Epoch 42/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 771.8120 - mean_squared_error: 771.8120\n",
      "Epoch 43/100\n",
      "17930/17930 [==============================] - 2s 84us/step - loss: 770.1286 - mean_squared_error: 770.1295\n",
      "Epoch 44/100\n",
      "17930/17930 [==============================] - 2s 86us/step - loss: 780.6550 - mean_squared_error: 780.6546\n",
      "Epoch 45/100\n",
      "17930/17930 [==============================] - 2s 89us/step - loss: 768.4134 - mean_squared_error: 768.4135\n",
      "Epoch 46/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 765.5805 - mean_squared_error: 765.5803\n",
      "Epoch 47/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 768.3561 - mean_squared_error: 768.3568\n",
      "Epoch 48/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 772.4814 - mean_squared_error: 772.4818\n",
      "Epoch 49/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 770.7253 - mean_squared_error: 770.7252\n",
      "Epoch 50/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 769.3685 - mean_squared_error: 769.3687\n",
      "Epoch 51/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 774.0887 - mean_squared_error: 774.0878\n",
      "Epoch 52/100\n",
      "17930/17930 [==============================] - 1s 82us/step - loss: 769.0356 - mean_squared_error: 769.0361\n",
      "Epoch 53/100\n",
      "17930/17930 [==============================] - 1s 81us/step - loss: 766.8821 - mean_squared_error: 766.8821\n",
      "Epoch 54/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 762.9088 - mean_squared_error: 762.9091\n",
      "Epoch 55/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 761.3120 - mean_squared_error: 761.3123\n",
      "Epoch 56/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 767.7379 - mean_squared_error: 767.7379\n",
      "Epoch 57/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 758.7733 - mean_squared_error: 758.7740\n",
      "Epoch 58/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 761.9733 - mean_squared_error: 761.9733\n",
      "Epoch 59/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 757.7290 - mean_squared_error: 757.7289\n",
      "Epoch 60/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 763.8570 - mean_squared_error: 763.8570\n",
      "Epoch 61/100\n",
      "17930/17930 [==============================] - 1s 82us/step - loss: 761.7709 - mean_squared_error: 761.7711\n",
      "Epoch 62/100\n",
      "17930/17930 [==============================] - 1s 82us/step - loss: 759.7980 - mean_squared_error: 759.7982\n",
      "Epoch 63/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 758.0760 - mean_squared_error: 758.0766\n",
      "Epoch 64/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 758.1916 - mean_squared_error: 758.1915\n",
      "Epoch 65/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 760.5447 - mean_squared_error: 760.5444\n",
      "Epoch 66/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 757.0408 - mean_squared_error: 757.0407\n",
      "Epoch 67/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 755.9743 - mean_squared_error: 755.9741\n",
      "Epoch 68/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 757.7220 - mean_squared_error: 757.7222\n",
      "Epoch 69/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 759.0467 - mean_squared_error: 759.0472\n",
      "Epoch 70/100\n",
      "17930/17930 [==============================] - 1s 82us/step - loss: 749.4751 - mean_squared_error: 749.4750\n",
      "Epoch 71/100\n",
      "17930/17930 [==============================] - 1s 80us/step - loss: 750.9417 - mean_squared_error: 750.9417\n",
      "Epoch 72/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 750.8101 - mean_squared_error: 750.8107\n",
      "Epoch 73/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 757.4415 - mean_squared_error: 757.4415\n",
      "Epoch 74/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 749.1713 - mean_squared_error: 749.1719\n",
      "Epoch 75/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 745.1248 - mean_squared_error: 745.1250\n",
      "Epoch 76/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 749.0153 - mean_squared_error: 749.0161\n",
      "Epoch 77/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 743.5260 - mean_squared_error: 743.5254\n",
      "Epoch 78/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 743.2274 - mean_squared_error: 743.2277\n",
      "Epoch 79/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 748.4898 - mean_squared_error: 748.4898\n",
      "Epoch 80/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 747.4570 - mean_squared_error: 747.4575\n",
      "Epoch 81/100\n",
      "17930/17930 [==============================] - 2s 88us/step - loss: 741.2481 - mean_squared_error: 741.2482\n",
      "Epoch 82/100\n",
      "17930/17930 [==============================] - 1s 81us/step - loss: 744.7297 - mean_squared_error: 744.7293\n",
      "Epoch 83/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 739.1451 - mean_squared_error: 739.1453\n",
      "Epoch 84/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 735.1198 - mean_squared_error: 735.1192\n",
      "Epoch 85/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 739.6955 - mean_squared_error: 739.6961\n",
      "Epoch 86/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 737.0720 - mean_squared_error: 737.0719\n",
      "Epoch 87/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 734.8062 - mean_squared_error: 734.8063\n",
      "Epoch 88/100\n",
      "17930/17930 [==============================] - 1s 79us/step - loss: 743.7570 - mean_squared_error: 743.7567\n",
      "Epoch 89/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 728.3630 - mean_squared_error: 728.3629\n",
      "Epoch 90/100\n",
      "17930/17930 [==============================] - 1s 82us/step - loss: 728.5181 - mean_squared_error: 728.5177\n",
      "Epoch 91/100\n",
      "17930/17930 [==============================] - 1s 81us/step - loss: 726.0671 - mean_squared_error: 726.0662\n",
      "Epoch 92/100\n",
      "17930/17930 [==============================] - 1s 81us/step - loss: 726.4243 - mean_squared_error: 726.4243\n",
      "Epoch 93/100\n",
      "17930/17930 [==============================] - 1s 81us/step - loss: 726.5093 - mean_squared_error: 726.5095\n",
      "Epoch 94/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 725.7120 - mean_squared_error: 725.7120\n",
      "Epoch 95/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 739.7733 - mean_squared_error: 739.7737\n",
      "Epoch 96/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 736.2448 - mean_squared_error: 736.2449\n",
      "Epoch 97/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 723.9865 - mean_squared_error: 723.9864\n",
      "Epoch 98/100\n",
      "17930/17930 [==============================] - 1s 78us/step - loss: 719.2814 - mean_squared_error: 719.2810\n",
      "Epoch 99/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 727.7662 - mean_squared_error: 727.7662\n",
      "Epoch 100/100\n",
      "17930/17930 [==============================] - 1s 77us/step - loss: 722.1189 - mean_squared_error: 722.1196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5e3c663760>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "\n",
    "big_data = pd.read_csv('airbnb_coded.2.0.csv')\n",
    "sc = sc()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
    "var = 'bathrooms'\n",
    "var2 = 'accommodates'\n",
    "var3 = 'cleaning_fee'\n",
    "var4 = 'bedrooms'\n",
    "var5 = 'beds'\n",
    "var6 = 'minimum_nights_x'\n",
    "var7 = 'maximum_nights'\n",
    "var8 = 'number_of_reviews_y'\n",
    "var9 = 'listing_class'\n",
    "var10 = 'price_x'\n",
    "var11 = 'bed_code'\n",
    "var12 = 'cancellation_policy_mapped'\n",
    "var13 = 'room_code'\n",
    "var14 = 'superhost'\n",
    "var15 = 'instant_book'\n",
    "var16 = 'review_scores_location'\n",
    "var17 = 'review_scores_rating'\n",
    "var18 = 'host_total_listings_count'\n",
    "var19 = 'availability_90'\n",
    "var20 = 'room_code'\n",
    "var21 = 'extra_people'\n",
    "var22 = 'property_type'\n",
    "var23 = 'review_scores_value'\n",
    "var24 = 'review_scores_cleanliness'\n",
    "var25 = 'review_scores_checkin'\n",
    "var26 = 'review_scores_communication'\n",
    "var27 = 'review_scores_value'\n",
    "var28 = 'guests_included'\n",
    "var29 = 'security_deposit'\n",
    "var30 = 'host_since'\n",
    "var31 = 'loc_class'\n",
    "var32 = 'profile_pic'\n",
    "var33 = 'amenities'\n",
    "\n",
    "big_data = big_data[big_data.price_x < 400]\n",
    "big_data = big_data[big_data.price_x > 8]\n",
    "\n",
    "data = big_data[[var, var2, var3, var4, var5, var6, var7, var8, var11, var12, var13, var14, var15, var16, var17, var18, var19, var21, var22, var23, var24,\n",
    "                 var25, var26, var27, var28, var29, var30, var31, var32, var33, var9, var10]]\n",
    "\n",
    "data[var] = data[var].fillna(data[var].median())\n",
    "data[var2] = data[var2].fillna(data[var2].median())\n",
    "data[var3] = data[var3].fillna(data[var3].median())\n",
    "data[var4] = data[var4].fillna(data[var4].median())\n",
    "data[var5] = data[var5].fillna(data[var5].median())\n",
    "\n",
    "data[var6][17990] = data[var6].median()\n",
    "data[var6][6215] = data[var6].median()\n",
    "data[var6][11299] = data[var6].median()\n",
    "data[var6][12737] = data[var6].median()\n",
    "data[var6][14598] = data[var6].median()\n",
    "# data[var7][872] = 8000\n",
    "# data[var7][2372] = 8000\n",
    "# data[var7][4391] = 8000\n",
    "# data[var7][17952] = 8000\n",
    "# data[var7][17990] = data[var7].median()\n",
    "# data[var7][6351] = data[var7].median()\n",
    "# data[var7][8341] = data[var7].median()\n",
    "\n",
    "\n",
    "data[var6] = data[var6].fillna(data[var6].median())\n",
    "data[var7] = data[var7].fillna(data[var7].median())\n",
    "data[var8] = data[var8].fillna(data[var8].median())\n",
    "data[var10] = data[var10].fillna(data[var10].median())\n",
    "data[var11] = data[var11].fillna(data[var11].median())\n",
    "data[var12] = data[var12].fillna(data[var12].median())\n",
    "data[var13] = data[var13].fillna(data[var13].median())\n",
    "data[var14] = data[var14].fillna(data[var14].median())\n",
    "data[var15] = data[var15].fillna(data[var15].median())\n",
    "data[var16] = data[var16].fillna(data[var16].median())\n",
    "data[var17] = data[var17].fillna(data[var17].median())\n",
    "data[var18] = data[var18].fillna(data[var18].median())\n",
    "data[var19] = data[var19].fillna(data[var19].median())\n",
    "data[var20] = data[var20].fillna(data[var20].median())\n",
    "data[var21] = data[var21].fillna(data[var21].median())\n",
    "data[var22] = data[var22].fillna('Apartment')\n",
    "data[var23] = data[var23].fillna(data[var23].median())\n",
    "data[var24] = data[var24].fillna(data[var24].median())\n",
    "data[var25] = data[var25].fillna(data[var25].median())\n",
    "data[var26] = data[var26].fillna(data[var26].median())\n",
    "data[var27] = data[var27].fillna(data[var27].median())\n",
    "data[var28] = data[var28].fillna(data[var28].median())\n",
    "data[var29] = data[var29].fillna(data[var29].median())\n",
    "data[var30] = data[var30].fillna(data[var30].median())\n",
    "data[var31] = data[var31].fillna(data[var31].median())\n",
    "data[var32] = data[var32].fillna(data[var32].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data[var6][17990] = data[var6].median()\n",
    "\n",
    "# data = data.drop(data.index[17990])\n",
    "data = data.drop(data.index[14598])\n",
    "\n",
    "data = data[[var2, var3, var6, var18, var19, var21, var8, var16, var17, var, var29,\n",
    "             var9, var13, var12, var31, var10]]\n",
    "# data = data[[var, var2, var3, var6, var8, var18, var17, var21, var19, var9, var12, var20, var22, var10]]\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "X = np.array(X)\n",
    "X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "X = np.array(X)\n",
    "X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "X = np.array(X)\n",
    "X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vect = CountVectorizer()\n",
    "# amenities = data[var33].values\n",
    "# # print(amenities)\n",
    "# # for i in range(len(amenities)):\n",
    "# # print(amenities)\n",
    "# vect.fit(amenities)\n",
    "# amenities_array = vect.transform(amenities)\n",
    "# amenities_array = amenities_array.todense()\n",
    "\n",
    "# X = np.concatenate((X, amenities_array),axis=1)\n",
    "# plt.scatter(data[var6].values, data[var7].values, s=100, c='red', alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "print(len(X[0]))\n",
    "print(X)\n",
    "# X[:,-8:] = sc.fit_transform(X[:,-8:])\n",
    "# X = sc.fit_transform(X)\n",
    "# print(X[0])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# data.head()\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "ann = Sequential()\n",
    "ann.add(Dense(output_dim=17, init='uniform', activation='relu', input_dim=26))\n",
    "ann.add(Dense(output_dim=17, init='uniform', activation='relu'))\n",
    "ann.add(Dense(output_dim=17, init='uniform', activation='relu'))\n",
    "ann.add(Dense(output_dim=17, init='uniform', activation='relu'))\n",
    "ann.add(Dense(output_dim=17, init='uniform', activation='relu'))\n",
    "ann.add(Dense(output_dim=1, init='uniform'))\n",
    "ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "ann.fit(X, y, batch_size=12, nb_epoch=100)\n",
    "# from xgboost import XGBRegressor\n",
    "# xgb = XGBRegressor(n_estimators=1000,learning_rate=0.01,max_depth=9,subsample=0.8)\n",
    "# xgb.fit(X, y)\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "[[  1.    0.    0.  ...  98.    1.  200. ]\n",
      " [  1.    0.    0.  ...  80.    1.  100. ]\n",
      " [  1.    0.    0.  ...  90.    1.5   0. ]\n",
      " ...\n",
      " [  0.    1.    0.  ... 100.    1.  350. ]\n",
      " [  1.    0.    0.  ... 100.    1.  100. ]\n",
      " [  1.    0.    0.  ... 100.    1.  200. ]]\n",
      "26\n",
      "[[  1.    0.    0.  ...  98.    1.  200. ]\n",
      " [  1.    0.    0.  ...  80.    1.  100. ]\n",
      " [  1.    0.    0.  ...  90.    1.5   0. ]\n",
      " ...\n",
      " [  0.    1.    0.  ... 100.    1.  350. ]\n",
      " [  1.    0.    0.  ... 100.    1.  100. ]\n",
      " [  1.    0.    0.  ... 100.    1.  200. ]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "\n",
    "test_data = pd.read_csv('airbnb_test_coded.csv')\n",
    "sc = sc()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
    "var = 'bathrooms'\n",
    "var2 = 'accommodates'\n",
    "var3 = 'cleaning_fee'\n",
    "var4 = 'bedrooms'\n",
    "var5 = 'beds'\n",
    "var6 = 'minimum_nights_x'\n",
    "var7 = 'maximum_nights'\n",
    "var8 = 'number_of_reviews_y'\n",
    "var9 = 'listing_class'\n",
    "var10 = 'price_x'\n",
    "var11 = 'bed_code'\n",
    "var12 = 'cancellation_policy_mapped'\n",
    "var13 = 'room_code'\n",
    "var14 = 'superhost'\n",
    "var15 = 'instant_book'\n",
    "var16 = 'review_scores_location'\n",
    "var17 = 'review_scores_rating'\n",
    "var18 = 'host_total_listings_count'\n",
    "var19 = 'availability_90'\n",
    "var20 = 'room_code'\n",
    "var21 = 'extra_people'\n",
    "var22 = 'property_type'\n",
    "var23 = 'review_scores_value'\n",
    "var24 = 'review_scores_cleanliness'\n",
    "var25 = 'review_scores_checkin'\n",
    "var26 = 'review_scores_communication'\n",
    "var27 = 'review_scores_value'\n",
    "var28 = 'guests_included'\n",
    "var29 = 'security_deposit'\n",
    "var30 = 'host_since'\n",
    "var31 = 'loc_class'\n",
    "var32 = 'profile_pic'\n",
    "var33 = 'amenities'\n",
    "\n",
    "# big_data = big_data[big_data.price_x < 500]\n",
    "\n",
    "data = test_data[[var, var2, var3, var4, var5, var6, var8, var11, var12, var13, var14, var15, var16, var17, var18, var19, var21, var22, var23, var24,\n",
    "                 var25, var26, var27, var28, var29, var30, var31, var32, var33, var9]]\n",
    "\n",
    "data[var] = data[var].fillna(data[var].median())\n",
    "data[var2] = data[var2].fillna(data[var2].median())\n",
    "data[var3] = data[var3].fillna(data[var3].median())\n",
    "data[var4] = data[var4].fillna(data[var4].median())\n",
    "data[var5] = data[var5].fillna(data[var5].median())\n",
    "\n",
    "data[var6][17990] = data[var6].median()\n",
    "data[var6][6215] = data[var6].median()\n",
    "data[var6][11299] = data[var6].median()\n",
    "data[var6][12737] = data[var6].median()\n",
    "data[var6][14598] = data[var6].median()\n",
    "# data[var7][872] = 8000\n",
    "# data[var7][2372] = 8000\n",
    "# data[var7][4391] = 8000\n",
    "# data[var7][17952] = 8000\n",
    "# data[var7][17990] = data[var7].median()\n",
    "# data[var7][6351] = data[var7].median()\n",
    "# data[var7][8341] = data[var7].median()\n",
    "\n",
    "\n",
    "data[var6] = data[var6].fillna(data[var6].median())\n",
    "# data[var7] = data[var7].fillna(data[var7].median())\n",
    "data[var8] = data[var8].fillna(data[var8].median())\n",
    "# data[var10] = data[var10].fillna(data[var10].median())\n",
    "data[var11] = data[var11].fillna(data[var11].median())\n",
    "data[var12] = data[var12].fillna(data[var12].median())\n",
    "data[var13] = data[var13].fillna(data[var13].median())\n",
    "data[var14] = data[var14].fillna(data[var14].median())\n",
    "data[var15] = data[var15].fillna(data[var15].median())\n",
    "data[var16] = data[var16].fillna(data[var16].median())\n",
    "data[var17] = data[var17].fillna(data[var17].median())\n",
    "data[var18] = data[var18].fillna(data[var18].median())\n",
    "data[var19] = data[var19].fillna(data[var19].median())\n",
    "data[var20] = data[var20].fillna(data[var20].median())\n",
    "data[var21] = data[var21].fillna(data[var21].median())\n",
    "data[var22] = data[var22].fillna('Apartment')\n",
    "data[var23] = data[var23].fillna(data[var23].median())\n",
    "data[var24] = data[var24].fillna(data[var24].median())\n",
    "data[var25] = data[var25].fillna(data[var25].median())\n",
    "data[var26] = data[var26].fillna(data[var26].median())\n",
    "data[var27] = data[var27].fillna(data[var27].median())\n",
    "data[var28] = data[var28].fillna(data[var28].median())\n",
    "data[var29] = data[var29].fillna(data[var29].median())\n",
    "data[var30] = data[var30].fillna(data[var30].median())\n",
    "data[var31] = data[var31].fillna(data[var31].median())\n",
    "data[var32] = data[var32].fillna(data[var32].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data[var6][17990] = data[var6].median()\n",
    "\n",
    "# data = data.drop(data.index[17990])\n",
    "\n",
    "data = data[[var2, var3, var6, var18, var19, var21, var8, var16, var17, var, var29,\n",
    "             var9, var13, var12, var31]]\n",
    "# data = data[[var, var2, var3, var6, var8, var18, var17, var21, var19, var9, var12, var20, var22, var10]]\n",
    "\n",
    "\n",
    "X = data.iloc[:, :].values\n",
    "# y = data.iloc[:, -1].values\n",
    "X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "X = np.array(X)\n",
    "X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "X = np.array(X)\n",
    "X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "X = np.array(X)\n",
    "X = ct.fit_transform(X)\n",
    "# X = X.todense()\n",
    "X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "# X = ct.fit_transform(X)\n",
    "# # X = X.todense()\n",
    "# X = np.array(X)\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vect = CountVectorizer()\n",
    "# amenities = data[var33].values\n",
    "# # print(amenities)\n",
    "# # for i in range(len(amenities)):\n",
    "# # print(amenities)\n",
    "# vect.fit(amenities)\n",
    "# amenities_array = vect.transform(amenities)\n",
    "# amenities_array = amenities_array.todense()\n",
    "\n",
    "# X = np.concatenate((X, amenities_array),axis=1)\n",
    "# plt.scatter(data[var6].values, data[var7].values, s=100, c='red', alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "print(len(X[0]))\n",
    "print(X)\n",
    "# X[:,-8:] = sc.fit_transform(X[:,-8:])\n",
    "# X = sc.fit_transform(X)\n",
    "# print(X[0])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# data.head()\n",
    "\n",
    "print(len(X[0]))\n",
    "print(X)\n",
    "# X[:,-8:] = sc.fit_transform(X[:,-8:])\n",
    "# X = sc.fit_transform(X)\n",
    "# print(X[0])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# data.head()\n",
    "\n",
    "\n",
    "y_pred = ann.predict(X)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47.375336],\n",
       "       [59.248165],\n",
       "       [30.338474],\n",
       "       ...,\n",
       "       [90.12008 ],\n",
       "       [66.00599 ],\n",
       "       [50.426754]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          1\n",
      "2          2\n",
      "3          3\n",
      "4          4\n",
      "        ... \n",
      "4506    4506\n",
      "4507    4507\n",
      "4508    4508\n",
      "4509    4509\n",
      "4510    4510\n",
      "Name: index, Length: 4511, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_data['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = test_data[['index', 'id']]\n",
    "save_data['Predicted values'] = y_pred\n",
    "save_data.to_csv('predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
